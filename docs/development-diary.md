17.11

- 2 phase processing for urls #12
- Make analyze use playwright #39
- Make analyze be able to use authenticated scraping #41

Not yet completed due to challenges, but getting there:

- Try gemini function calling #36
- Make crawling more robust #40
- UX for grannys #42
- Retry gemini request 2x #43

What is planned next:
- Demo preparations
- Finish above unfinished tasks

## 10.11

- Alternative extract mode #27
- Testing basic functionality on different sites #28
- Testing auth scraping #29
- Crawling along different next page links #13

Not yet completed due to challenges, but getting there:

- 2 phase processing for urls #12

What is planned next:

- Handle SPA better #34
- Make analyze use playwright #39
- Make analyze be able to use authenticated scraping #41
- Try gemini function calling #36
- Make crawling more robust #40
- UX for grannys #42
- Retry gemini request 2x #43

## 3.11

- Improve authenticated scraping #22
- Optimize the agent(s) #23

Not yet completed due to challenges, but getting there:

- 2 phase processing for urls #12
- Crawling along different next page links #13

What is planned next:

- Alternative extract mode #27
- Testing basic functionality on different sites #28
- Testing auth scraping #29

## 27.10

- Experiement with subagent #11
- Setup the pipeline #15
- Authenticated scraping #14

Not yet completed due to challenges, but getting there:

- 2 phase processing for urls #12
- Crawling along different next page links #13

What is planned next:

- Improve authenticated scraping #22
- Optimize the agent(s) #23

## 20.10

Autumn break

## 13.10

- Init agent to handle spider data #3
- Test and integrate #8

What is planned next:

- Experiement with subagent #11
- 2 phase processing for urls #12
- Crawling along different next page links #13
- Authenticated scraping #14
- Setup the pipeline #15

## 6.10

- Init spiders #1
- Extension: capturing urls #6
- Extension: Validate and send #7

What is planned next:

- Init agent to handle spider data #3
- Test and integrate #8

## 29.9

- Setup the project backend (Python+FastAPI) #2
- Setup extension WXT (React+TS) #5
- Experiment with spiders
- Experiment with agents

What is planned next:

- Init spiders #1
- Init agent to handle spider data #3
- Extension: capturing urls #6
- Extension: Validate and send #7
- Test and integrate #8