# `routes_process.py` Module

This module contains the core logic for processing a batch of URLs, generating scraping instructions via an AI agent, and returning the scraped data.

The process is started by `process_urls` and broken down into three main functions:

### 1. `process_urls(request)`

This is the main API endpoint function that gets called when the server receives a `/api/process` request. It acts as an orchestrator and operates in two phases:

*   **Phase 1: Selector Generation:** It creates a list of concurrent tasks, one for each initial URL provided in the request. Each task calls `generate_selectors_for_url` to get a unique set of selectors for that specific URL. It waits for all these tasks to complete.
*   **Phase 2: Scraping and Crawling:** Once all selectors are generated, it creates a second list of concurrent tasks. Each task calls `scrape_and_crawl`, passing it an initial URL and its corresponding set of selectors from Phase 1.
*   **Phase 3: Formatting:** After all scraping is finished, it gathers all the data, formats it into a single CSV string, and returns it as a downloadable file to the browser extension.

### 2. `generate_selectors_for_url(url, prompt)`

This asynchronous helper function is responsible for handling a single URL during Phase 1. It:
1.  Fetches the HTML content of the given `url`.
2.  Sends the HTML and the user's `prompt` to the Gemini agent.
3.  Returns the unique set of selectors generated by the agent for that page.

### 3. `scrape_and_crawl(url, selectors, depth)`

This asynchronous and recursive function handles the scraping for a specific URL path during Phase 2. It:
1.  Uses a **pre-generated** set of `selectors` to scrape data from the current `url`.
2.  Checks for a special `next_page_selector` within the selectors.
3.  If a "next page" link is found and `depth` allows, it calls itself to crawl to the next page, reusing the same set of selectors.
4.  Returns a list of all data scraped along its crawl path.